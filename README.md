# Fake news challenge - the winning solution
This is the project for ut claim checking group to run the winning solution for fake new challenge.<br>
For the original winning solution, check the GitHub link [here](https://github.com/Cisco-Talos/fnc-1).<br>
For the fake news challenge website, please check [here](http://www.fakenewschallenge.org/).<br>

## Relevant Readings
- The State of Automated Factchecking Easy read.
- Emergent: a novel data-set for stance classification: dataset in the fake news challenge, has stance labels and ‘truth’ labels. (300 claims,  and 2,595 associated article headlines; 47.7% for, 15.2% against and 37.1% observing) http://www.emergent.info/ The “headline” is the real headline. Some stances are subjective, could be a problem when crowdsource that.
- An Interpretable Joint Graphical Model for Fact-Checking from Crowds Our paper.
- Detect Rumors in Microblog Posts Using Propagation Structure via Kernel Learning
- ACL 2017 short paper: “Liar, Liar Pants on Fire”: A New Benchmark Dataset for Fake News Detection. William Yang Wang DATA
- Another ACL 2017 short paper: Separating Facts from Fiction: Linguistic Models to Classify Suspicious and Trusted News Posts on Twitter. Kyle Shaffer, Jin Yea Jang, Nathan Hodas and Svitlana Volkova
Fact Checking: Task definition and dataset construction From PolitiFact, has ‘verdict’ labels and ‘suitable’ labels (can it be objectively checked)
- Where the Truth Lies: Explaining the Credibility of Emerging Claims on the Web and Social Media From Snopes and Wikipedia: DATA WWW’17
- Leveraging the crowd to detect and reduce the spread of fake news and misinformation
- Detecting Fake News in Social Networks via Crowdsourcing
- Truth of varying shades: Analyzing language in fake news and political fact-checking
- Some Like it Hoax: Automated Fake News Detection in Social Networks code

## Contact
For any installation issues, please contact Chiahui by email (chiahuiliu@utexas.edu)

